{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P3nYMs_7yaGV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lTaxHnJym3t"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "letmbbK3zA-7",
        "outputId": "bd92070a-78f9-4d2b-dee1-8704243acd32"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"gdown\" no se reconoce como un comando interno o externo,\n",
            "programa o archivo por lotes ejecutable.\n"
          ]
        }
      ],
      "source": [
        "if not Path(\"data_extended.json\").is_file():\n",
        "    !gdown \"www.dropbox.com/ ... a_extended.json?dl=1 \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9SV8zEqzgec"
      },
      "outputs": [],
      "source": [
        "with open(\"data_extended.json\", \"r\") as f:\n",
        "    data_json = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Ib8T8m02zA"
      },
      "outputs": [],
      "source": [
        "divide_into = 4\n",
        "\n",
        "X = torch.tensor(data_json[\"mfcc\"])\n",
        "y = torch.tensor(data_json[\"labels\"])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "if divide_into is not None:\n",
        "    X_train = X_train.unsqueeze(1).reshape(X_train.shape[0], divide_into, -1, 32).reshape(X_train.shape[0] * divide_into, -1, 32)\n",
        "    y_train = np.repeat(y_train, divide_into)\n",
        "    X_val = X_val.unsqueeze(1).reshape(X_val.shape[0], divide_into, -1, 32).reshape(X_val.shape[0] * divide_into, -1, 32)\n",
        "    y_val = np.repeat(y_val, divide_into)\n",
        "    X_test = X_test.unsqueeze(1).reshape(X_test.shape[0], divide_into, -1, 32).reshape(X_test.shape[0] * divide_into, -1, 32)\n",
        "    y_test = np.repeat(y_test, divide_into)\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWcnurJBRghi"
      },
      "outputs": [],
      "source": [
        "def show_mfcc(example, example_class):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "    sns.heatmap(example.T, ax=ax)\n",
        "    ax.set_xlabel(\"Time\")\n",
        "    ax.set_ylabel(\"Frequency\")\n",
        "    ax.set_title(f\"Class: {data_json['mapping'][1:][example_class]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkOYjxoeyTju"
      },
      "source": [
        "# Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib0sQk0G34sr"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rnn_type,\n",
        "        n_input_channels,\n",
        "        hidd_size=256,\n",
        "        num_layers=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Para utilizar una vanilla RNN entregue rnn_type=\"RNN\"\n",
        "        Para utilizar una LSTM entregue rnn_type=\"LSTM\"\n",
        "        Para utilizar una GRU entregue rnn_type=\"GRU\"\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "\n",
        "        if rnn_type == \"GRU\":\n",
        "            self.rnn_layer = nn.GRU(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
        "        \n",
        "        elif rnn_type == \"LSTM\":\n",
        "            self.rnn_layer = nn.LSTM(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
        "\n",
        "        elif rnn_type == \"RNN\":\n",
        "            self.rnn_layer = nn.RNN(n_input_channels, hidd_size, batch_first=True, num_layers=num_layers)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"rnn_type {rnn_type} not supported.\")\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidd_size, 10),\n",
        "        )\n",
        "\n",
        "        self.flatten_layer = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.rnn_type == \"GRU\":\n",
        "            out, h = self.rnn_layer(x)\n",
        "\n",
        "        elif self.rnn_type == \"LSTM\":\n",
        "            out, (h, c) = self.rnn_layer(x)\n",
        "\n",
        "        elif self.rnn_type == \"RNN\":\n",
        "            out, h = self.rnn_layer(x)\n",
        "            \n",
        "        out = h[-1]\n",
        "\n",
        "        return self.net(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIMXo8EC4JoX"
      },
      "outputs": [],
      "source": [
        "class CNN1DModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidd_size=256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            # COMPLETAR CÓDIGO\n",
        "        )\n",
        "\n",
        "        self.rnn_layer = RNNModel(\n",
        "            n_input_channels= , # COMPLETAR CÓDIGO\n",
        "            rnn_type=\"LSTM\",\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # COMPLETAR CÓDIGO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2_irgYB0K-i"
      },
      "source": [
        "# Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B_myMRgLjZR"
      },
      "outputs": [],
      "source": [
        "def show_curves(all_curves):\n",
        "\n",
        "    final_curve_means = {k: np.mean([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
        "    final_curve_stds = {k: np.std([c[k] for c in all_curves], axis=0) for k in all_curves[0].keys()}\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
        "    fig.set_facecolor('white')\n",
        "\n",
        "    epochs = np.arange(len(final_curve_means[\"val_loss\"])) + 1\n",
        "\n",
        "    ax[0].plot(epochs, final_curve_means['val_loss'], label='validation')\n",
        "    ax[0].plot(epochs, final_curve_means['train_loss'], label='training')\n",
        "    ax[0].fill_between(epochs, y1=final_curve_means[\"val_loss\"] - final_curve_stds[\"val_loss\"], y2=final_curve_means[\"val_loss\"] + final_curve_stds[\"val_loss\"], alpha=.5)\n",
        "    ax[0].fill_between(epochs, y1=final_curve_means[\"train_loss\"] - final_curve_stds[\"train_loss\"], y2=final_curve_means[\"train_loss\"] + final_curve_stds[\"train_loss\"], alpha=.5)\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_title('Loss evolution during training')\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, final_curve_means['val_acc'], label='validation')\n",
        "    ax[1].plot(epochs, final_curve_means['train_acc'], label='training')\n",
        "    ax[1].fill_between(epochs, y1=final_curve_means[\"val_acc\"] - final_curve_stds[\"val_acc\"], y2=final_curve_means[\"val_acc\"] + final_curve_stds[\"val_acc\"], alpha=.5)\n",
        "    ax[1].fill_between(epochs, y1=final_curve_means[\"train_acc\"] - final_curve_stds[\"train_acc\"], y2=final_curve_means[\"train_acc\"] + final_curve_stds[\"train_acc\"], alpha=.5)\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "    ax[1].set_ylabel('Accuracy')\n",
        "    ax[1].set_title('Accuracy evolution during training')\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def get_metrics_and_confusion_matrix(model, dataset):\n",
        "    model.cpu()\n",
        "    model.eval()\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=min(16, len(dataset)))\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    for x, y in dataloader:\n",
        "        y_true.append(y)\n",
        "        y_pred.append(model(x).argmax(dim=1))\n",
        "\n",
        "    y_true = torch.cat(y_true)\n",
        "    y_pred = torch.cat(y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy_score(y_true, y_pred) * 100:.2f}%\")\n",
        "    \n",
        "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=data_json[\"mapping\"][1:], xticks_rotation=\"vertical\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSHsRNpn4IiC"
      },
      "outputs": [],
      "source": [
        "def train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu):\n",
        "    # Predicción\n",
        "    y_predicted = model(x_batch)\n",
        "\n",
        "    # Cálculo de loss\n",
        "    loss = criterion(y_predicted, y_batch)\n",
        "\n",
        "    # Actualización de parámetros\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return y_predicted, loss\n",
        "\n",
        "\n",
        "def evaluate(val_loader, model, criterion, use_gpu):\n",
        "    cumulative_loss = 0\n",
        "    cumulative_predictions = 0\n",
        "    data_count = 0\n",
        "\n",
        "    for x_val, y_val in val_loader:\n",
        "        if use_gpu:\n",
        "            x_val = x_val.cuda()\n",
        "            y_val = y_val.cuda()\n",
        "\n",
        "        y_predicted = model(x_val)\n",
        "        \n",
        "        loss = criterion(y_predicted, y_val)\n",
        "\n",
        "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "\n",
        "        cumulative_predictions += (y_val == class_prediction).sum().item()\n",
        "        cumulative_loss += loss.item() * y_val.shape[0]\n",
        "        data_count += y_val.shape[0]\n",
        "\n",
        "    val_acc = cumulative_predictions / data_count\n",
        "    val_loss = cumulative_loss / data_count\n",
        "\n",
        "    return val_acc, val_loss\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    epochs,\n",
        "    criterion,\n",
        "    batch_size,\n",
        "    lr,\n",
        "    n_evaluations_per_epoch=6,\n",
        "    use_gpu=False,\n",
        "):\n",
        "    if use_gpu:\n",
        "        model.cuda()\n",
        "\n",
        "    # Definición de dataloader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=use_gpu)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=use_gpu)\n",
        "\n",
        "    # Optimizador\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "    # Listas para guardar curvas de entrenamiento\n",
        "    curves = {\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "    }\n",
        "\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    n_batches = len(train_loader)\n",
        "    print(n_batches)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\rEpoch {epoch + 1}/{epochs}\")\n",
        "        cumulative_train_loss = 0\n",
        "        cumulative_train_corrects = 0\n",
        "        examples_count = 0\n",
        "\n",
        "        # Entrenamiento del modelo\n",
        "        model.train()\n",
        "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            if use_gpu:\n",
        "                x_batch = x_batch.cuda()\n",
        "                y_batch = y_batch.cuda()\n",
        "\n",
        "            y_predicted, loss = train_step(x_batch, y_batch, model, optimizer, criterion, use_gpu)\n",
        "\n",
        "            cumulative_train_loss += loss.item() * x_batch.shape[0]\n",
        "            examples_count += y_batch.shape[0]\n",
        "\n",
        "            # Calculamos número de aciertos\n",
        "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "            cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
        "\n",
        "            if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
        "                train_loss = cumulative_train_loss / examples_count\n",
        "                train_acc = cumulative_train_corrects / examples_count\n",
        "\n",
        "                print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_acc, val_loss = evaluate(val_loader, model, criterion, use_gpu)\n",
        "\n",
        "        print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
        "\n",
        "        train_loss = cumulative_train_loss / examples_count\n",
        "        train_acc = cumulative_train_corrects / examples_count\n",
        "\n",
        "        curves[\"train_acc\"].append(train_acc)\n",
        "        curves[\"val_acc\"].append(val_acc)\n",
        "        curves[\"train_loss\"].append(train_loss)\n",
        "        curves[\"val_loss\"].append(val_loss)\n",
        "\n",
        "    print()\n",
        "    total_time = time.perf_counter() - t0\n",
        "    print(f\"Tiempo total de entrenamiento: {total_time:.4f} [s]\")\n",
        "\n",
        "    model.cpu()\n",
        "\n",
        "    return curves, total_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zAojTEUsSi4"
      },
      "outputs": [],
      "source": [
        "lr = 5e-4\n",
        "batch_size = 32\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "n_trains = 5\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "all_curves = []\n",
        "times = []\n",
        "\n",
        "for train_run in range(n_trains):\n",
        "    model = # INSERTE CÓDIGO\n",
        "\n",
        "    curves, total_time = train_model(\n",
        "        model,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        epochs,\n",
        "        criterion,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        use_gpu=True,\n",
        "    )\n",
        "\n",
        "    all_curves.append(curves)\n",
        "    times.append(total_time)\n",
        "\n",
        "print(f\"Tiempo de entrenamiento promedio de {n_trains} corridas: {np.mean(times):.2f} +- {np.std(times):.2f} [s]\")\n",
        "\n",
        "show_curves(all_curves)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "e654902f1877883cfc0f8194d1615c44937abe63f3bb0e0105192e55863f0170"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}